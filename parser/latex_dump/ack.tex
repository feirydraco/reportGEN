\thispagestyle{empty}
\begin{center}
\textup{\large{\textbf{ACKNOWLEDGEMENTS}}} \\[0.1in]
\end{center}

\justify
\indent
Convolutional neural networks (CNNs) are commonly developed at a fixed resource cost, and then scaled up in order to achieve better accuracy when more resources are made available. For example, ResNet can be scaled up from ResNet-18 to ResNet-200 by increasing the number of layers, and recently, GPipe achieved 84.3% ImageNet top-1 accuracy by scaling up a baseline CNN by a factor of four. 
\medskip
The conventional practice for model scaling is to arbitrarily increase the CNN depth or width, or to use larger input image resolution for training and evaluation. While these methods do improve accuracy, they usually require tedious manual tuning, and still often yield suboptimal performance. 
\medskip
What if, instead, we could find a more principled method to scale up a CNN to obtain better accuracy and efficiency?


\vfill

\justify
\begin{tabularx}{\linewidth}{X X}
 & {\hfill}\textup{ }\\ 
  & {\hfill}\textup{ }\\ 
 \textup{Date:} & {\hfill}\textup{ }\\
\textup{Place:} & {\hfill}\textup{Yash Vora 1RN16CS123}\\
\end{tabularx}


\pagebreak
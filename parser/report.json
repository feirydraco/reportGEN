{"modifications": {"colors": [{"colorname": "darkbrown", "RGB": [128, 0, 0]}, {"colorname": "blue", "RGB": [0, 51, 204]}, {"colorname": "black", "RGB": [0, 0, 0]}], "packages": ["xcolor", "tikz", "multicol", "titletoc", "ragged2e", "fancyhdr", "url", "verbatim", "float", "titlesec", "listings", "mathptmx", "adjustbox", "titlesec", "tabularx"]}, "coverpage": {"title": {"subject": {"topic": "Technical Seminar", "title": "EfficientNet: Rethinking Model Scaling for CNNs", "semester": "8th"}, "students": {"members": [{"Name": "Yash Vora", "USN": "1RN16CS123"}, {"Name": "", "USN": ""}, {"Name": "", "USN": ""}, {"Name": "", "USN": ""}]}, "teachers": {"members": [{"Name": "", "Designation": "", "Department": ""}, {"Name": "Mr. Devraju B.M.", "Designation": " Assistant Professor", "Department": "Dept. of CSE"}, {"Name": "", "Designation": "", "Department": ""}]}}, "certificate": {"members": [{"Name": "Mr. Devraju B.M.", "Designation": " Assistant Professor", "Department": "Dept. of CSE"}, {"Name": "Dr. G. T. Raju", "Designation": "Vice Principal", "Department": "Professor and HoD"}, {"Name": "Dr. M. K. Venkatesha", "Designation": "Principal", "Department": ""}]}, "acknowledgements": {"content": "The joy and satisfaction that accompany the successful completion of any task would be incomplete without thanking those who made it possible. I consider myself proud to be a part of RNS Institute of Technology, the institution which moulded me in all my endeavours. I express my gratitude to beloved Chairman **Dr. R N Shetty**, for providing state of art facilities. I express my deep gratitude to **Dr. H N Shivashankar**, Director, who has always been a great source of inspiration.\r\n\r\n<BR>\r\n\r\nI would like to express my sincere thanks to **Dr. M K Venkatesha**, Principal and **Dr. G.T. Raju**, Vice Principal, Professor and HOD, Department of CSE, for their valuable guidance and encouragement throughout. I extend my sincere thanks and heartfelt gratitude to my Technical seminar reviewer **Mr. Devraju B.M.**, Asst. Prof., Department of CSE.\r\n\r\n<BR>\r\n\r\nFinally, I take this opportunity to extend my earnest gratitude and respect to my parents, teaching and non-teaching staff of the department, the library staff and all my friends who have directly or indirectly supported me.\r\n"}, "abstract": {"content": "Convolutional neural networks (CNNs) are commonly developed at a fixed resource cost, and then scaled up in order to achieve better accuracy when more resources are made available. For example, ResNet can be scaled up from ResNet-18 to ResNet-200 by increasing the number of layers, and recently, GPipe achieved 84.3% ImageNet top-1 accuracy by scaling up a baseline CNN by a factor of four. \r\n\r\n<BR>\r\n\r\nThe conventional practice for model scaling is to arbitrarily increase the CNN depth or width, or to use larger input image resolution for training and evaluation. While these methods do improve accuracy, they usually require tedious manual tuning, and still often yield suboptimal performance. \r\n\r\n<BR>\r\n\r\nWhat if, instead, we could find a more principled method to scale up a CNN to obtain better accuracy and efficiency?\r\n\r\n"}}, "chapters": [{"name": "Introduction", "number": 1, "content": [{"type": "text", "id": 1, "content": "#Background#\r\nConvolutional Neural Networks are widely used to achieve better accuracy in various deep learning and computer vision tasks. Since AlexNet won the 2012 ImageNet competition, CNNs (short for Convolutional Neural Networks) have become the de facto algorithms for a wide variety of tasks in deep learning, especially for computer vision. From 2012 to date, researchers have been experimenting and trying to come up with better and better architectures to improve models accuracy on different tasks. They are primarily developed on a fixed budget and scaled up to meet requirement later. The seminar details creation of efficient models as the competition is to beat the top accuracy; Scaling, if done correctly, can also help in improving the efficiency of a model.\r\n\r\n#Definitions#\r\nHere the topics are described and defined which are later required in the technical seminar.\r\n##Convolutional Neural Networks (CNN)##\r\nIn deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series.\r\n\r\nCNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.\r\n\r\nConvolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field."}, {"type": "image", "id": 2, "content": "ConvNet.jpg"}, {"type": "text", "id": 3, "content": "##Hyperparameters##\r\nIn machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.\r\n\r\nHyperparameters can be classified as model hyperparameters, that cannot be inferred while fitting the machine to the training set because they refer to the model selection task, or algorithm hyperparameters, that in principle have no influence on the performance of the model but affect the speed and quality of the learning process. An example of a model hyperparameter is the topology and size of a neural network. Examples of algorithm hyperparameters are learning rate and mini-batch size.\r\n\r\n##Grid Search##\r\nThe traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\r\n\r\nSince the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search."}, {"type": "image", "id": 4, "content": "Scaling parameters of CNN.png"}, {"type": "text", "id": 5, "content": "##Scaling##\r\nThere are three scaling dimensions of a CNN: depth, width, and resolution. Depth simply means how deep the networks is which is equivalent to the number of layers in it. Width simply means how wide the network is. One measure of width, for example, is the number of channels in a Conv layer whereas Resolution is simply the image resolution that is being passed to a CNN. Intuitively, the compound scaling method makes sense because if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image."}]}, {"name": "Literature Survey", "number": 2, "content": [{"type": "text", "id": 1, "content": "#Deep Residual Learning for Image Recognition (2015)#\r\nDeeper neural networks are more difficult to train. The authors present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. They explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. Then the authors provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. Hence the authors provide significant evidence showing increasing depth of the network results in higher accuracy.\r\n\r\n#Wide Residual Networks (2017)#\r\nDeeper residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper the authors conduct a detailed experimental study on the architecture of ResNet blocks, based on which they proposed a novel architecture where they decreased the depth and increased width of residual networks. The authors call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. The authors successfully demonstrate that making a network wider increases accuracy.\r\n\r\n#GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism (2016)#\r\nScaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, the authors introduced GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. This paper demonstrated that the number of channels of a network also affect the accuracy of the model.\r\n"}]}, {"name": "Problem Statement", "number": 3, "content": [{"type": "text", "id": 1, "content": "#Statement#\r\nThe problem statement can be reduced to the following."}, {"type": "image", "id": 2, "content": "Problem Definition.png"}, {"type": "text", "id": 3, "content": "The ConvNet can also be demonstrated as a series of layers, with the following form. In practice, ConvNet layers are often partitioned into multiple stages and all layers in each stage share the same architecture: for example, ResNet has five stages, and all layers in each stage has the same convolutional type except the first layer performs down-sampling. Therefore, we can define a ConvNet as:\r\n"}, {"type": "image", "id": 4, "content": "Layer-wise representation of CNN.png"}, {"type": "text", "id": 5, "content": "Unlike regular model scaling, this method tries expand network length(Li), width(Ci), and/or resolution(Hi, Wi) without changing Fi. By fixing Fi, task of model scaling is simplified but there still remains a large design space to explore Li, Ci, Wi, Hi, for each layer. In order to further reduce design space, the authors restrict that all layers must be scaled uniformly with constant ratio. Hence the target is to maximize model accuracy for any given resource constraint, which can be formulated as the following optimization problem.\r\n\r\n\r\n\r\n"}, {"type": "image", "id": 6, "content": "Optimization Problem.png"}]}, {"name": "Architecture", "number": 4, "content": [{"type": "text", "id": 1, "content": "#Compound Scaling#\r\nThe authors proposed a simple yet very effective scaling technique which uses a compound coefficient Phi to uniformly scale network width, depth, and resolution in a principled way:"}, {"type": "image", "id": 2, "content": "Compound Scaling.png"}, {"type": "text", "id": 3, "content": "Phi is a user-specified coefficient that controls how many more resources are available for model scaling. In a CNN, Conv layers are the most compute expensive part of the network. Also, FLOPS of a regular convolution op is almost proportional to d, w squared, r squared, i.e. doubling the depth will double the FLOPS while doubling width or resolution increases FLOPS almost by four times. Hence, in order to make sure that the total FLOPS don\u2019t exceed Phi squared. The constraint applied is that the product of alpha, beta and gamma squared should be nearly equal to 2."}, {"type": "text", "id": 4, "content": "#EfficientNet Architecture#\r\nScaling doesn\u2019t change the layer operations, hence it is better to first have a good baseline network and then scale it along different dimensions using the proposed compound scaling. The authors obtained their base network by doing a Neural Architecture Search (NAS) that optimizes for both accuracy and FLOPS. The architecture is similar to M-NASNet as it has been found using the similar search space. The network layers/blocks are as shown below:"}, {"type": "image", "id": 5, "content": "EfficientNet baseline network.png"}, {"type": "text", "id": 6, "content": "Now we have the base network, we can search for optimal values for our scaling parameters. If you revisit the equation, you will quickly realize that we have a total of four parameters to search for: alpha, beta, gamma and Phi. In order to make the search space smaller and making the search operation less costly, the search for these parameters can be completed in two steps.\r\n<LIST>\r\n~ Fix Phi as 1, assuming that twice more resources are available, and do a small grid search for alpha, beta and gamma. For baseline network B0, it turned out the optimal values are alpha as 1.2, beta as 1.1, and gamma as 1.15 such that product of alpha, beta and gamma squared is almost equal to 2.\r\n~ Now fix alpha, beta and gamma as constants (with values found in above step) and experiment with different values of Phi. The different values of Phi produce EfficientNets B1-B7.\r\n<LIST>"}]}, {"name": "Experiments and Results", "number": 5, "content": [{"type": "text", "id": 1, "content": "#Scaling#\r\nScaling Up a Baseline Model with Different Network Width (w), Depth (d), and Resolution (r) Coefficients. Bigger networks with larger width, depth, or resolution tend to achieve higher accuracy, but the accuracy gain quickly saturate after reaching 80%, demonstrating the limitation of single dimension scaling. Baseline network is described as described below.\r\n"}, {"type": "image", "id": 2, "content": "Depth Scaling.png"}, {"type": "text", "id": 3, "content": "Scaling Network Width for Different Baseline Networks. Each dot in a line denotes a model with different width coefficient (w). All baseline networks are from the previous table. The first baseline network (d=1.0, r=1.0) has 18 convolutional layers with resolution 224x224, while the last baseline (d=2.0, r=1.3) has 36 layers with resolution 299x299.\r\n"}, {"type": "image", "id": 4, "content": "Scaling Depth and Resolution.png"}, {"type": "text", "id": 5, "content": "#Comparison of EffecientNet#\r\nAs seen in the figure, the scaled models use upto 8.4x less parameters and a total reduction of upto 16x FLOPS is observed. The models were compared head-to-head against state of the art networks in their respective tasks.\r\n"}, {"type": "image", "id": 6, "content": "Results.png"}, {"type": "text", "id": 7, "content": "It is also very evident that the compound scaling method also improves CNN dependability as we can see that the CNN is now focusing on the required objects for the task in a more accurate manner. This is done by freezing the training at an intermediate step and then observing one of the layers of the CNN. This demonstrates the ability of this algorithm to also apply to transfer learning tasks with good performance. Hence we can use these modified networks in mobile devices with less computing power but with the same efficiency of the full size model."}, {"type": "image", "id": 8, "content": "What the network can see.png"}, {"type": "text", "id": 9, "content": "#Limitations#\r\nIt is possible to achieve even better performance by searching for alpha, beta and gamma directly around a large model, but the search cost becomes prohibitively more expensive on larger model. Hence we would require much higher computation power for doing such a task.\r\n"}]}], "ref": [{"caption": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "link": "https://arxiv.org/abs/1905.11946"}, {"caption": "Deep Residual Learning for Image Recognition", "link": "https://arxiv.org/abs/1512.03385"}, {"caption": "Wide Residual Networks", "link": "https://arxiv.org/abs/1605.07146"}, {"caption": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism", "link": "https://arxiv.org/abs/1811.06965"}, {"caption": "On the Expressive Power of Deep Neural Networks", "link": "https://arxiv.org/abs/1606.05336"}, {"caption": "The Expressive Power of Neural Networks: A View from the Width", "link": "https://arxiv.org/abs/1709.02540"}]}